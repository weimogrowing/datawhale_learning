# 预训练与后训练区别

- 预训练是重新训练一个大模型，要用到众多数据集，且非常消耗训练资源和时间

# 后训练方法

- 在指定领域的相对小数据集上微调，训练更快且成本更低
- 常用下面三种后训练方法

  - **监督微调（SFT）**

    - 直接用带标注的数据监督微调，让其学会与人类对话；***注意预训练主要是token级别的训练，主要优化token，而后训练是增强LLM的对话能力***
  - **直接偏好优化（DPO）**

    - 需要创建提示-响应数据集，向模型展示优质和劣质答案，相当于对比学习，能让模型趋向于优质答案
  - **在线强化学习(Online RL）**

    - 只给出提示集，让模型输出多个回答，利用奖励函数对回答质量评分，模型根据奖励分数更新
    - 获取奖励函数的方法

      - 额外训练一个与人类评判标准相当的模型
      - 利用数学验证器或单元测试判定给定的答案是否正确，正确性度量作为奖励函数

# 成功的后训练需要确保三个关键要素

- **数据与算法的协同设计**
- **可靠高效的算法库**
- **合适的评估体系**
  提升单一基准成绩相对容易，但要在不损害其他领域能力的前提下改进特定行为则更具挑战

  - 基于人类偏好的聊天评估
  - LLM评估
  - 指令模型静态基准
  - 知识与推理数据集
  - 指令遵循评估
  - 函数调用与智能体评估

# 大模型后训练场景

- 最简单的方法：提示词工程
- 稍微复杂：RAG
- 领域专用模型：持续预训练+后训练
- 当需要严格遵循20条以上指令，或提升特定能力时，后训练最能体现价值
