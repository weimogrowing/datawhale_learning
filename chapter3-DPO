DPO通过处理多个候选对比来学习用户偏好，而不是依赖于具体的数值奖励。例如，给定一组对话生成的候选答案，DPO根据哪个答案更符合人类的期望来调整模型的输出。

DPO的损失函数通常涉及到对比学习，即通过优化使得正确的候选（偏好较高的选项）被模型的评分值提升，而较差的候选则被降分。这种方法能直接反映出模型对于偏好的理解。
损失函数：
 L DPO = − log ⁡ σ ( β ( log ⁡ π θ ( y pos ∣ x ) π ref ( y pos ∣ x ) − log ⁡ π θ ( y neg ∣ x ) π ref ( y neg ∣ x ) ) ) 
